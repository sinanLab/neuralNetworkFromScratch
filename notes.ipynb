{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b618c24",
   "metadata": {},
   "source": [
    "Here's the complete explanation with proper Markdown and LaTeX formatting that you can directly copy into a Jupyter Notebook:\n",
    "\n",
    "# Neural Network from Scratch: Mathematical Foundations\n",
    "\n",
    "## 1. Network Architecture\n",
    "\n",
    "### Layer Dimensions:\n",
    "- **Input layer**: 2 neurons (for 2D input data)  \n",
    "- **Hidden layer**: $n_h$ neurons (configurable)  \n",
    "- **Output layer**: 1 neuron (single output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf3552",
   "metadata": {},
   "source": [
    "```\n",
    "Input → Hidden Layer → Output\n",
    "(x₁,x₂)     h        ŷ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d103f12",
   "metadata": {},
   "source": [
    "## 2. Forward Propagation\n",
    "\n",
    "### Hidden Layer Calculation\n",
    "$$\n",
    "\\mathbf{h} = \\sigma(\\mathbf{X}\\mathbf{W}_1 + \\mathbf{b}_1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{m \\times 2}$: Input matrix (m samples)\n",
    "- $\\mathbf{W}_1 \\in \\mathbb{R}^{2 \\times n_h}$: Weight matrix\n",
    "- $\\mathbf{b}_1 \\in \\mathbb{R}^{1 \\times n_h}$: Bias vector\n",
    "- $\\sigma$: Sigmoid activation\n",
    "\n",
    "### Output Layer Calculation\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\sigma(\\mathbf{h}\\mathbf{W}_2 + \\mathbf{b}_2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W}_2 \\in \\mathbb{R}^{n_h \\times 1}$: Weight matrix\n",
    "- $\\mathbf{b}_2 \\in \\mathbb{R}^{1 \\times 1}$: Bias scalar\n",
    "\n",
    "### Sigmoid Activation\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### Sigmoid Derivative\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "## 3. Loss Function (MSE)\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{m}\\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b6d7c",
   "metadata": {},
   "source": [
    "## 4. Backpropagation\n",
    "\n",
    "### Output Layer Gradients\n",
    "Error at output:\n",
    "$$\n",
    "\\mathbf{\\delta}_2 = (\\mathbf{y} - \\mathbf{\\hat{y}}) \\odot \\sigma'(\\mathbf{\\hat{y}})\n",
    "$$\n",
    "\n",
    "Weight gradients:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2} = \\mathbf{h}^T \\mathbf{\\delta}_2\n",
    "$$\n",
    "\n",
    "Bias gradient:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_2} = \\sum \\mathbf{\\delta}_2 \\text{ (across samples)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7e0f5",
   "metadata": {},
   "source": [
    "### Hidden Layer Gradients\n",
    "Error propagation:\n",
    "$$\n",
    "\\mathbf{\\delta}_1 = (\\mathbf{\\delta}_2 \\mathbf{W}_2^T) \\odot \\sigma'(\\mathbf{h})\n",
    "$$\n",
    "\n",
    "Weight gradients:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1} = \\mathbf{X}^T \\mathbf{\\delta}_1\n",
    "$$\n",
    "\n",
    "Bias gradient:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_1} = \\sum \\mathbf{\\delta}_1 \\text{ (across samples)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3bccb",
   "metadata": {},
   "source": [
    "## 5. Parameter Updates\n",
    "\n",
    "Gradient descent updates:\n",
    "$$\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{b} \\leftarrow \\mathbf{b} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967398d5",
   "metadata": {},
   "source": [
    "## 6. Training Algorithm Steps\n",
    "\n",
    "1. **Initialize** all weights and biases\n",
    "2. **Forward pass**:\n",
    "   - Compute hidden layer outputs\n",
    "   - Compute final predictions\n",
    "3. **Calculate loss** between predictions and true values\n",
    "4. **Backward pass**:\n",
    "   - Compute output layer gradients\n",
    "   - Propagate error to hidden layer\n",
    "   - Compute hidden layer gradients\n",
    "5. **Update parameters** using gradient descent\n",
    "6. **Repeat** for specified number of epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551cf78",
   "metadata": {},
   "source": [
    "## Implementation Notes\n",
    "\n",
    "- All equations are implemented using NumPy array operations\n",
    "- Matrix dimensions must align properly at each step\n",
    "- The sigmoid activation introduces non-linearity\n",
    "- MSE loss penalizes large errors quadratically\n",
    "\n",
    "This Markdown content with LaTeX equations will render perfectly in Jupyter Notebook. The equations will display as properly formatted mathematical notation when the notebook is run.\n",
    "\n",
    "To use:\n",
    "1. Create a new Markdown cell in Jupyter\n",
    "2. Paste this entire content\n",
    "3. Run the cell to see beautifully formatted equations and explanations\n",
    "\n",
    "You can intersperse these explanation cells with code cells containing the actual implementation for a complete learning experience."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
